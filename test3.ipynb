{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try to use the data to train a model\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, TimeDistributed, Bidirectional, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import math\n",
    "from tensorflow.keras import regularizers\n",
    "import gc\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "\n",
    "train_size_per = 80     # set the percentage of training data\n",
    "                        # the validation and test data will be split equally from the remaining data\n",
    "                        # e.g. if train_size_per = 70\n",
    "                        # 70% of the data is used for training\n",
    "                        # 15% of the data is used for validation\n",
    "                        # 15% of the data is used for testing\n",
    "                        \n",
    "rotation_per_position = 16  # set the number of rotations per position used on the matlab file\n",
    "\n",
    "reads_per_position = 1000   # set the number of reads per rotation used on the matlab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the file\n",
    "\n",
    "SensorValues = pd.read_csv('mySensorData.csv', header = None)\n",
    "LocationValues = pd.read_csv('myLocationData.csv', header = None)\n",
    "\n",
    "\n",
    "sensor_arr = SensorValues.values\n",
    "location_arr = LocationValues.values\n",
    "\n",
    "\n",
    "print(sensor_arr.shape)\n",
    "print(location_arr.shape)\n",
    "\n",
    "\n",
    "print(np.min(sensor_arr))\n",
    "print(np.max(sensor_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_min_max_scaling(data, min_val, max_val):\n",
    "    arr = (max_val - min_val) * data + min_val\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(data):\n",
    "    # Calculate the minimum and maximum values of the data\n",
    "    \n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    \n",
    "    scaled_data = (data - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the Location of the real data points\n",
    "\n",
    "plt.scatter(location_arr[:,0], location_arr[:,1], color='blue', label='Real Points')\n",
    "\n",
    "# # Set the x-axis label to \"X\"\n",
    "plt.xlabel('X')\n",
    "\n",
    "# # Set the y-axis label to \"Y\"\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# # Set the title of the plot to \"Real vs Predicted Points\"\n",
    "plt.title('Full dataset points')\n",
    "\n",
    "# # Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_values_per_position = rotation_per_position * reads_per_position              \n",
    "\n",
    "\n",
    "X = min_max_scaler(sensor_arr)\n",
    "Y_loc_0 = location_arr[:,0]\n",
    "Y_loc_1 = location_arr[:,1]\n",
    "Y_rot = location_arr[:,2]\n",
    "\n",
    "Y = np.zeros((Y_loc_0.shape[0], 3))\n",
    "\n",
    "for i in range(0, Y_loc_0.shape[0]):\n",
    "    Y[i,0] = Y_loc_0[i]\n",
    "    Y[i,1] = Y_loc_1[i]\n",
    "    Y[i,2] = Y_rot[i]\n",
    "\n",
    "\n",
    "data_sen_size = int(X.shape[0])\n",
    "data_sen_size_2 = int(X.shape[1])\n",
    "\n",
    "data_loc_size = int(Y.shape[0])\n",
    "data_loc_size_2 = int(Y.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "val_size = int(data_sen_size * (((100 - train_size_per)/100) * 1/2))\n",
    "test_size = int(data_sen_size * (((100 - train_size_per)/100) * 1/2))\n",
    "train_size = int(data_sen_size * (train_size_per/100))\n",
    "\n",
    "print(\"train_size: \", train_size)\n",
    "print(\"val_size: \", val_size)\n",
    "print(\"test_size: \", test_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size_rotation = int(data_sen_size / total_values_per_position)\n",
    "\n",
    "val_test_size_per = int(data_size_rotation * ((100 - train_size_per)/100))\n",
    "\n",
    "random_numbers = random.sample(range(0, data_size_rotation), val_test_size_per)\n",
    "\n",
    "random.shuffle(random_numbers)\n",
    "\n",
    "halfway = len (random_numbers) // 2\n",
    "\n",
    "test_random = random_numbers[:halfway]\n",
    "val_random = random_numbers[halfway:]\n",
    "\n",
    "val_numbers = np.zeros((val_size, 1))\n",
    "test_numbers = np.zeros((test_size, 1))\n",
    "\n",
    "n_val = 0\n",
    "for i in val_random:\n",
    "    for j in range(0, total_values_per_position):\n",
    "        val_numbers[n_val] = i * total_values_per_position + j\n",
    "        n_val = n_val + 1\n",
    "\n",
    "n_test = 0\n",
    "for i in test_random:\n",
    "    for j in range(0, total_values_per_position):\n",
    "        test_numbers[n_test] = i * total_values_per_position + j\n",
    "        n_test = n_test + 1\n",
    "        \n",
    "val_sorted = np.sort(val_numbers, axis=0)\n",
    "test_sorted = np.sort(test_numbers, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Commented out the code below if you want to use the data from the csv files with the train,val and test positions\n",
    " \n",
    "######################################################################\n",
    "#\n",
    "# with open('mySortedVal153.csv', 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     for row in val_sorted:\n",
    "#         writer.writerow(row)\n",
    "#\n",
    "# with open('mySortedTest153.csv', 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     for row in test_sorted:\n",
    "#         writer.writerow(row)   \n",
    "#\n",
    "######################################################################\n",
    "\n",
    "# Comment the code bellow if you want to save the train,val and test positions to the .csv files\n",
    "\n",
    "######################################################################\n",
    "\n",
    "SortedVal = pd.read_csv('mySortedVal153.csv', header = None)\n",
    "SortedTest = pd.read_csv('mySortedTest153.csv', header = None)\n",
    "\n",
    "val_sorted= SortedVal.values\n",
    "test_sorted= SortedTest.values\n",
    "\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((train_size, data_sen_size_2))\n",
    "Y_train = np.zeros((train_size, data_loc_size_2))\n",
    "\n",
    "X_val = np.zeros((val_size, data_sen_size_2))\n",
    "Y_val = np.zeros((val_size, data_loc_size_2))\n",
    "\n",
    "X_test = np.zeros((test_size, data_sen_size_2))\n",
    "Y_test = np.zeros((test_size, data_loc_size_2))\n",
    "\n",
    "n_test = 0\n",
    "n_train = 0\n",
    "n_val = 0\n",
    "\n",
    "for i in range (0, data_sen_size):  \n",
    "    if i in val_sorted:\n",
    "        X_val[n_val] = X[i]\n",
    "        Y_val[n_val] = Y[i]\n",
    "        n_val = n_val + 1\n",
    "    elif i in test_sorted:\n",
    "        X_test[n_test] = X[i]\n",
    "        Y_test[n_test] = Y[i]\n",
    "        n_test = n_test + 1\n",
    "    else:\n",
    "        X_train[n_train] = X[i]\n",
    "        Y_train[n_train] = Y[i]\n",
    "        n_train = n_train + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "plt.scatter(Y_train[:,0], Y_train[:,1], color='blue', label='Real Points')\n",
    "\n",
    "# # Set the x-axis label to \"X\"\n",
    "plt.xlabel('X')\n",
    "\n",
    "# # Set the y-axis label to \"Y\"\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# # Set the title of the plot to \"Real vs Predicted Points\"\n",
    "plt.title('Y train dataset points')\n",
    "\n",
    "# # Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_val[:,0], Y_val[:,1], color='blue', label='Real Points')\n",
    "\n",
    "# # Set the x-axis label to \"X\"\n",
    "plt.xlabel('X')\n",
    "\n",
    "# # Set the y-axis label to \"Y\"\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# # Set the title of the plot to \"Real vs Predicted Points\"\n",
    "plt.title('Y val dataset points')\n",
    "\n",
    "# # Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test[:,0], Y_test[:,1], color='blue', label='Real Points')\n",
    "\n",
    "# # Set the x-axis label to \"X\"\n",
    "plt.xlabel('X')\n",
    "\n",
    "# # Set the y-axis label to \"Y\"\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# # Set the title of the plot to \"Real vs Predicted Points\"\n",
    "plt.title('Y test dataset points')\n",
    "\n",
    "# # Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_train[:,0], Y_train[:,1], color='blue', label='Train')\n",
    "plt.scatter(Y_val[:,0], Y_val[:,1], color='red', label='Validation')\n",
    "plt.scatter(Y_test[:,0], Y_test[:,1], color='green', label='Test')\n",
    "\n",
    "# # Set the x-axis label to \"X\"\n",
    "plt.xlabel('X (meters)')\n",
    "\n",
    "# # Set the y-axis label to \"Y\"\n",
    "plt.ylabel('Y (meters)')\n",
    "\n",
    "# # Set the title of the plot to \"Real vs Predicted Points\"\n",
    "plt.title('Dataset points')\n",
    "\n",
    "# # Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning - Euclidean Loss Functions for X and Y\n",
    "\n",
    "def loss_function_xy_euclidean(y_true, y_pred):\n",
    "    xy_true = y_true[:, :2]\n",
    "    xy_pred = y_pred[:, :2]\n",
    "    y_true = xy_true[:, 1]\n",
    "    x_true = xy_true[:, 0]\n",
    "    y_pred = xy_pred[:, 1]\n",
    "    x_pred = xy_pred[:, 0]\n",
    "    distance = tf.reduce_mean(K.sqrt(K.square(y_pred - y_true) + K.square(x_pred - x_true)))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning - Euclidean Metric Functions for X and Y\n",
    "\n",
    "def metric_function_xy_euclidean (y_true, y_pred):\n",
    "    xy_true = y_true[:, :2]\n",
    "    xy_pred = y_pred[:, :2]\n",
    "    y_true = xy_true[:, 1]\n",
    "    x_true = xy_true[:, 0]\n",
    "    y_pred = xy_pred[:, 1]\n",
    "    x_pred = xy_pred[:, 0]\n",
    "    distance = tf.reduce_mean(K.sqrt(K.square(y_pred - y_true) + K.square(x_pred - x_true)))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning - Loss Functions for theta\n",
    "\n",
    "def loss_function_theta(y_true, y_pred):\n",
    "    delta_angles = y_pred - y_true\n",
    "    # Wrap delta_angles to the [-pi, pi] range\n",
    "    wrapped_diff = tf.atan2(tf.sin(delta_angles), tf.cos(delta_angles))\n",
    "    return tf.reduce_mean(tf.square(wrapped_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_function_theta(y_true, y_pred):\n",
    "    delta_angles = y_pred - y_true\n",
    "    # Wrap delta_angles to the [-pi, pi] range\n",
    "    wrapped_diff = tf.atan2(tf.sin(delta_angles), tf.cos(delta_angles))\n",
    "    return tf.reduce_mean(K.abs(wrapped_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_xy_mse(y_true, y_pred):\n",
    "    xy_true = y_true[:, :2]\n",
    "    xy_pred = y_pred[:, :2]\n",
    "    y_true = xy_true[:, 1]\n",
    "    x_true = xy_true[:, 0]\n",
    "    y_pred = xy_pred[:, 1]\n",
    "    x_pred = xy_pred[:, 0]\n",
    "    distance = tf.reduce_mean(K.square(K.sqrt(K.square(y_pred - y_true) + K.square(x_pred - x_true))))\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_xy_mae(y_true, y_pred):\n",
    "    xy_true = y_true[:, :2]\n",
    "    xy_pred = y_pred[:, :2]\n",
    "    y_true = xy_true[:, 1]\n",
    "    x_true = xy_true[:, 0]\n",
    "    y_pred = xy_pred[:, 1]\n",
    "    x_pred = xy_pred[:, 0]\n",
    "    distance = tf.reduce_mean(K.abs(K.sqrt(K.square(y_pred - y_true) + K.square(x_pred - x_true))))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_xy_huber(y_true, y_pred, delta=1.0):\n",
    "    xy_true = y_true[:, :2]\n",
    "    xy_pred = y_pred[:, :2]\n",
    "    x_true = xy_true[:, 0]\n",
    "    y_true = xy_true[:, 1]\n",
    "    x_pred = xy_pred[:, 0]\n",
    "    y_pred = xy_pred[:, 1]\n",
    "    error = tf.reduce_mean(K.sqrt(K.square(y_pred - y_true) + K.square(x_pred - x_true)))\n",
    "    is_small_error = K.abs(error) < delta\n",
    "    squared_loss = 0.5 * K.square(error)\n",
    "    linear_loss = delta * (K.abs(error) - 0.5 * delta)\n",
    "    return tf.reduce_mean(tf.where(is_small_error, squared_loss, linear_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_theta_mse(y_true, y_pred):\n",
    "    delta_angles = y_pred - y_true\n",
    "    wrapped_diff = tf.atan2(tf.sin(delta_angles), tf.cos(delta_angles))\n",
    "    return tf.reduce_mean(K.square(wrapped_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_theta_mae(y_true, y_pred):\n",
    "    delta_angles = y_pred - y_true\n",
    "    wrapped_diff = tf.atan2(tf.sin(delta_angles), tf.cos(delta_angles))\n",
    "    return tf.reduce_mean(K.abs(wrapped_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_theta_huber(y_true, y_pred, delta=1.0):\n",
    "    delta_angles = y_pred - y_true\n",
    "    wrapped_diff = tf.atan2(tf.sin(delta_angles), tf.cos(delta_angles))\n",
    "    error = wrapped_diff\n",
    "    is_small_error = K.abs(error) < delta\n",
    "    squared_loss = 0.5 * K.square(error)\n",
    "    linear_loss = delta * (K.abs(error) - 0.5 * delta)\n",
    "    return tf.reduce_mean(tf.where(is_small_error, squared_loss, linear_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early Stopping\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10,  # Stop training if no improvement after 10 epochs\n",
    "    restore_best_weights=True  # Restore the best model weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Checkpoint\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Schedule\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 2:\n",
    "        return 0.0001\n",
    "    elif epoch < 10:\n",
    "        return 0.00001\n",
    "    elif epoch < 20:\n",
    "        return 0.000001\n",
    "    elif epoch < 40:\n",
    "        return 0.0000001\n",
    "    elif epoch < 75:\n",
    "        return 0.00000001\n",
    "    else:\n",
    "        return 0.000000001\n",
    "    \n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard Logging\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='logs',\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize the data\n",
    "\n",
    "data = list(zip(X_train, Y_train))\n",
    "random.shuffle(data)\n",
    "\n",
    "X_train, Y_train = zip(*data)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning - Build the model\n",
    "\n",
    "inputs = tf.keras.Input(shape=(data_sen_size_2,))\n",
    "\n",
    "x = tf.keras.layers.Dense(480, activation='relu')(inputs)\n",
    "x = tf.keras.layers.Dense(960, activation='relu')(x)\n",
    "# x = tf.keras.layers.Dense(1440, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "# # x = tf.keras.layers.Dropout(0.1)(x)  # Add Dropout layer with a dropout rate of 0.5\n",
    "# x = tf.keras.layers.BatchNormalization()(x)  # Add BatchNormalization layer\n",
    "x = tf.keras.layers.Dense(480, activation='relu')(x)\n",
    "xy_output = tf.keras.layers.Dense(2, activation='linear', name='xy')(x)\n",
    "theta_output = tf.keras.layers.Dense(1, activation='linear', name='theta')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=[xy_output, theta_output])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.00001)\n",
    "model.compile(optimizer = optimizer, loss = {'xy': loss_function_xy_huber, 'theta': loss_function_theta_mae}, loss_weights={'xy': 0.5, 'theta': 0.5}, metrics = {'xy': metric_function_xy_euclidean, 'theta': metric_function_theta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning - Train the model\n",
    "predict = model.fit(\n",
    "    X_train, \n",
    "    {\n",
    "        \"xy\": Y_train[:, :2], \n",
    "        \"theta\": Y_train[:, 2]\n",
    "    }, \n",
    "    validation_data=(X_val, {\"xy\": Y_val[:, :2], \"theta\": Y_val[:, 2]}), \n",
    "    epochs=100, \n",
    "    batch_size=512, \n",
    "    verbose = 1, \n",
    "    callbacks=[\n",
    "        model_checkpoint,\n",
    "        lr_scheduler,\n",
    "        early_stopping\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the loss values from the history object\n",
    "training_loss = predict.history['loss']\n",
    "validation_loss = predict.history['val_loss']\n",
    "\n",
    "# Plotting the loss values\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "model.save('./models/Best_models/model.5x6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning - Evaluate the model\n",
    "\n",
    "losses = model.evaluate(X_train, {'xy': Y_train[:, :2], 'theta': Y_train[:, 2]}, batch_size=512)\n",
    "print(\"Mean Squared Error For Train Dataset: \", losses)\n",
    "gc.collect()\n",
    "losses = model.evaluate(X_val, {'xy': Y_val[:, :2], 'theta': Y_val[:, 2]}, batch_size=32)\n",
    "print(\"Mean Squared Error For Validation Dataset: \", losses)\n",
    "gc.collect()\n",
    "losses = model.evaluate(X_test, {'xy': Y_test[:, :2], 'theta': Y_test[:, 2]}, batch_size=32)\n",
    "print(\"Mean Squared Error For Test Dataset: \", losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning - Make predictions\n",
    "\n",
    "y_pred, theta_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_x = y_pred[:, 0]\n",
    "y_pred_y = y_pred[:, 1]\n",
    "y_pred_theta = theta_pred\n",
    "\n",
    "# Round the predicted values to the nearest integer\n",
    "\n",
    "\n",
    "# Combine the predicted x and y values into a single array of (x, y) tuples\n",
    "y_pred = np.stack((y_pred_x, y_pred_y), axis=1)\n",
    "\n",
    "# Calculate accuracy score\n",
    "mse = metrics.mean_squared_error(Y_test[:,:2], y_pred)\n",
    "\n",
    "print(\"Mean Error for test data (distance): \", math.sqrt(mse), \"meters/meters\" )\n",
    "\n",
    "mse = metrics.mean_squared_error(Y_test[:, 2], y_pred_theta)\n",
    "print(\"Mean squared error for test data (degrees): \", math.sqrt(mse), \"radians/radians\" )\n",
    "\n",
    "print(\"#############################################################################################################\")\n",
    "\n",
    "y_pred_theta = y_pred_theta.squeeze()\n",
    "\n",
    "delta_angles = Y_test[:, 2] - y_pred_theta\n",
    "# Wrap delta_angles to the [-pi, pi] range\n",
    "wrapped_diff = tf.atan2(tf.sin(delta_angles), tf.cos(delta_angles))\n",
    "theta_err = tf.reduce_mean(K.abs(wrapped_diff))\n",
    "print(\"Mean Error for test data (degrees): \", theta_err, \"radians/radians\" )\n",
    "\n",
    "err = tf.reduce_mean(tf.sqrt(tf.square(Y_test[:, 0] - y_pred[:, 0]) + tf.square(Y_test[:, 1] - y_pred[:, 1])))\n",
    "print(\"Mean Error for test data (distance): \", err, \"meters\")\n",
    "\n",
    "distance = (tf.sqrt(tf.square(Y_test[:, 0] - y_pred[:, 0]) + tf.square(Y_test[:, 1] - y_pred[:, 1])))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(len(distance))\n",
    "\n",
    "ax.scatter(range(len(distance)), distance)\n",
    "\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('Error (meters)')\n",
    "ax.set_title('Sample Error plot')\n",
    "\n",
    "\n",
    "# Show lines between adjacent samples\n",
    "for i in range(len(distance)-1):\n",
    "    ax.plot([i, i+1], [distance[i], distance[i+1]], )\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# Assuming residuals_x and residuals_y are calculated as shown in the previous example\n",
    "maxNumber = np.max(distance)\n",
    "multiple = maxNumber / 10\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(distance, bins=10, color='b', alpha=1, range=(0, maxNumber))\n",
    "plt.xlabel('Residuals (True - Predicted)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Error Histogram')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "plt.gca().xaxis.set_major_locator(MultipleLocator(multiple))\n",
    "plt.gca().yaxis.set_major_locator(MultipleLocator(distance.shape[0]/50))\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "maxNumber = np.max(distance)\n",
    "multiple = maxNumber / 10\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(distance, bins=10, color='b', alpha=1, cumulative=True, density=True, range=(0, maxNumber))\n",
    "plt.xlabel('Residuals (True - Predicted)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('CDF Histogram')\n",
    "\n",
    "plt.gca().xaxis.set_major_locator(MultipleLocator(multiple))\n",
    "plt.gca().yaxis.set_major_locator(MultipleLocator(0.1))\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_true contains the true positions and y_pred contains the predicted positions\n",
    "# Calculate the R-squared value\n",
    "\n",
    "y_pred, theta_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_x = y_pred[:, 0]\n",
    "y_pred_y = y_pred[:, 1]\n",
    "y_pred_theta = theta_pred\n",
    "\n",
    "# Round the predicted values to the nearest integer\n",
    "# Combine the predicted x and y values into a single array of (x, y) tuples\n",
    "y_pred = np.stack((y_pred_x, y_pred_y), axis=1)\n",
    "\n",
    "r_squared = r2_score(Y_test[:, :2], y_pred)\n",
    "\n",
    "print(\"R-squared (R^2):\", r_squared)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming y_true contains the true positions and y_pred contains the predicted positions\n",
    "residuals_x = Y_test[:, 0] - y_pred[:, 0]\n",
    "residuals_y = Y_test[:, 1] - y_pred[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred, theta_pred = model.predict(X_test)\n",
    "\n",
    "x_pred = y_pred[:, 0]\n",
    "y_pred = y_pred[:, 1]\n",
    "\n",
    "# Create a scatter plot showing the predicted points in red\n",
    "# plt.scatter(x_pred, y_pred, color='red', label='Predicted Points')\n",
    "number_for_plot = int(x_pred.shape[0]/4)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "print(number_for_plot)\n",
    "# Loop through each point in the test set\n",
    "for i in range(0 , 32000):\n",
    "    k = i * 10\n",
    "    # percentage = (i/number_for_plot)*100\n",
    "    # print(percentage)\n",
    "    # Get the x and y coordinates for the real and predicted points\n",
    "    x_real, y_real, theta_real = Y_test[k]\n",
    "    x_pred_arrow = x_pred[k]\n",
    "    y_pred_arrow = y_pred[k]\n",
    "    plt.scatter(x_real, y_real, color='blue')\n",
    "    plt.scatter(x_pred_arrow, y_pred_arrow, color='red')\n",
    "    # Add an arrow from the real point to the predicted point\n",
    "    # plt.arrow(x_real, y_real, x_pred_arrow - x_real, y_pred_arrow - y_real, \n",
    "    #           length_includes_head=True, head_width=0.01, color='green')\n",
    "    \n",
    "# Set the x-axis label to \"X\"\n",
    "plt.xlabel('X(meters)')\n",
    "\n",
    "# Set the y-axis label to \"Y\"\n",
    "plt.ylabel('Y(meters)')\n",
    "\n",
    "# Set the title of the plot to \"Real vs Predicted Points\"\n",
    "plt.title('Real vs Predicted Points')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
